---
title: "Text"
lang: "en-US"
draft: false
description: "Learn about how to set up a VDP Text component https://github.com/instill-ai/instill-core"
---

The Text component is an operator component that allows users to extract and manipulate text from different sources.
It can carry out the following tasks:

- [Chunk Text](#chunk-text)



## Release Stage

`Alpha`



## Configuration

The component configuration is defined and maintained [here](https://github.com/instill-ai/component/blob/main/operator/text/v0/config/definition.json).





## Supported Tasks

### Chunk Text

Chunk text with different strategies


| Input | ID | Type | Description |
| :--- | :--- | :--- | :--- |
| Task ID (required) | `task` | string | `TASK_CHUNK_TEXT` |
| Text (required) | `text` | string | Text to be chunked |
| Chunk Strategy (required) | `strategy` | object | Chunking strategy |
| Tokenization (required) | `tokenization` | object | Tokenization choices |



| Output | ID | Type | Description |
| :--- | :--- | :--- | :--- |
| Token Count | `token-count` | integer | Total count of tokens in the original input text |
| Text Chunks | `text-chunks` | array[object] | Text chunks after splitting |
| Number of Text Chunks | `chunk-num` | integer | Total number of output text chunks |
| Token Count Chunks | `chunks-token-count` | integer | Total count of tokens in the output text chunks |


### Chunking Strategy
There are three strategies available for chunking text in Text Component:
- 1. Token
- 2. Recursive
- 3. Markdown

#### Token
Language models have a token limit. You should not exceed the token limit. When you split your text into chunks it is therefore a good idea to count the number of tokens. There are many tokenizers. When you count tokens in your text you should use the same tokenizer as used in the language model.

| **Parameter**        | **Type**         | **Description**                                                           |
| -------------------- | ---------------- | ------------------------------------------------------------------------- |
| `chunk-size`         | integer          | Specifies the maximum size of each chunk in terms of the number of tokens |
| `chunk-overlap`      | integer          | Determines the number of tokens that overlap between consecutive chunks   |
| `allowed-special`    | array of strings | A list of special tokens that are allowed within chunks                   |
| `disallowed-special` | array of strings | A list of special tokens that should not appear within chunks             |

#### Recursive
This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is ["\n\n", "\n", " ", ""]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.

| **Parameter**    | **Type**         | **Description**                                                                              |
| ---------------- | ---------------- | -------------------------------------------------------------------------------------------- |
| `chunk-size`     | integer          | Specifies the maximum size of each chunk in terms of the number of tokens                    |
| `chunk-overlap`  | integer          | Determines the number of tokens that overlap between consecutive chunks                      |
| `separators`     | array of strings | A list of strings representing the separators used to split the text                         |
| `keep-separator` | boolean          | A flag indicating whether to keep the separator characters at the beginning or end of chunks |


#### Markdown
This text splitter is specially designed for Markdown format.

| **Parameter**   | **Type** | **Description**                                                           |
| --------------- | -------- | ------------------------------------------------------------------------- |
| `chunk-size`    | integer  | Specifies the maximum size of each chunk in terms of the number of tokens |
| `chunk-overlap` | integer  | Determines the number of tokens that overlap between consecutive chunks   |
| `code-blocks`   | boolean  | A flag indicating whether code blocks should be treated as a single unit  |

### Tokenization
There are 2 ways to choose the tokenizer:
- 1. Use Model name to choose the tokenizer
- 2. Use Encoding name to choose the tokenizer

#### Model Name

| **Model**                     |
| ----------------------------- |
| gpt-4o                        |
| gpt-4                         |
| gpt-3.5-turbo                 |
| command-r-plus                |
| command-r                     |
| command                       |
| command-nightly               |
| command-light                 |
| command-light-nightly         |
| embed-english-v3.0            |
| embed-multilingual-v3.0       |
| embed-english-light-v3.0      |
| embed-multilingual-light-v3.0 |
| text-davinci-003              |
| text-davinci-002              |
| text-davinci-001              |
| text-curie-001                |
| text-babbage-001              |
| text-ada-001                  |
| davinci                       |
| curie                         |
| babbage                       |
| ada                           |
| code-davinci-002              |
| code-davinci-001              |
| code-cushman-002              |
| code-cushman-001              |
| davinci-codex                 |
| cushman-codex                 |
| text-davinci-edit-001         |
| code-davinci-edit-001         |
| text-embedding-ada-002        |
| text-similarity-davinci-001   |
| text-similarity-curie-001     |
| text-similarity-babbage-001   |
| text-similarity-ada-001       |
| text-search-davinci-doc-001   |
| text-search-curie-doc-001     |
| text-search-babbage-doc-001   |
| text-search-ada-doc-001       |
| code-search-babbage-code-001  |
| code-search-ada-code-001      |
| gpt2                          |



#### Encoding Name
| **Encoding** |
| ------------ |
| o200k_base   |
| cl100k_base  |
| p50k_base    |
| r50k_base    |
| p50k_edit    |


### Text Chunks in Output
| **Parameter**    | **Type** | **Description**                                              |
| ---------------- | -------- | ------------------------------------------------------------ |
| `test`           | string   | The text chunk                                               |
| `start-position` | integer  | The starting position of the text chunk in the original text |
| `end-position`   | integer  | The ending position of the text chunk in the original text   |





